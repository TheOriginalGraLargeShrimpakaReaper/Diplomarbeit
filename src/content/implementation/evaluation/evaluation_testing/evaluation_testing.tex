%! Author = gramic
%! Date = 08.04.24

% Preamble
\begin{flushleft}
    \subsection{Testing Evaluationssysteme}
    %\input{content/implementation/evaluation/evaluation_testing/evaluation_testcases}
    %\input{content/implementation/evaluation/evaluation_testing/yugabyteDB}
    \subsubsection{Patroni}
    Patroni funktionierte wie gewollt.\\
    Da kein Connection Pooler auf dem Proxy-Host installiert wurde, kam nicht alles erfüllt werden.\\
    Wichtig dazu ist zu sagen, dass die REST-API und das Command vollständig funktioniert.\\
    \input{content/latex_tables/evaluation_tests_patroni}
\end{flushleft}
\begin{flushleft}
    \subsubsection{StackGres -Citus}
    StackGres kann nicht alle Anforderungen erfüllen.\\
    Obwohl es mit \texttt{envoy} und \texttt{pgBouncer} einen Proxy und einen Connection Pooler gibt,\\
    scheint dies nicht über die Coordinator-Nodes selbst zu gehen.\\
    Daher brechen bestehende Connections ab oder laufen irgendwann in ein Timeout, wenn \Gls{Kubernetes} Nodes nicht schnell genug heruntergefahren werden.
\end{flushleft}
\begin{flushleft}
    Aufgrund des Sharding und das in sich geschlossenen \Gls{Kubernetes}-Environments, wurde auf separate Tablespaces verzichtet.
\end{flushleft}
\begin{flushleft}
    Zuerst wurde versucht, das Sharding mit Version 12 eingeführte Schema Based Sharding umzusetzen.\\
    Wie beim Benchmarking auch, zeigten sich schnell die Grenzen des Citrus-Sharding.\\
    Sobald ein Foreign-Key zwischen zwei Tabellen, die in verschiedenen Schemas liegen, existiert, kann kein Schema Based Sharding mehr ausgeführt werden.\\
    Auch hier besteht die Lösung darin, Reference Tables zu erstellen.
\end{flushleft}
\begin{flushleft}
    \input{content/latex_tables/evaluation_tests_stackgres_citus}
    Die genauen Details sind im Anhang zu finden:
    \hyperref[subsec:appendix_testing_stackgres_citus]{Anhang - StackGres - Citus Testing}
\end{flushleft}
\begin{flushleft}
    \subsubsection{YugabyteDB}
    YugabyteDB funktionierte so weit.\\
    \input{content/latex_tables/evaluation_tests_yugabytedb}
    Was es aber bei einer Testinstallation zu prüfen gilt, ist die Zeiteinstellung.
\end{flushleft}
\begin{flushleft}
    Während dem Testing kam es immer wieder vor, dass ein Node Probleme mit der Zeit bekam.\\
    Dies fiel immer dann auf, wenn ein Node (meistens \texttt{sks1184}), heruntergefahren und später rebooted wurde.\\
    Der Fehler trat auch erst auf, als die Nodes aus einem Grund aus einem Snapshot wiederhergestellt werden mussten.\\
    YugabyteDB stellt dann oft mehr als 500ms Zeitunterschied zwischen dem Tablet-Leader und dem Follower fest.\\
    Sobald dies zutritt, ist der Server Node nicht mehr arbeitsfähig da die Zeit für die Synchronisation der Daten benötigt wird\cite{BYH9Z3MS}.\\
    Oft kam auch die Meldung, dass \texttt{chronyc} nicht mehr auf dem Pod installiert sei.\\
    Auf dem Servern scheinen die Zeiten aber synchron zu sein, eine genaue Ursache konnte nicht gefunden werden.\\
    Eine mögliche Ursache ist eine unsaubere Konfiguration von \gls{rke2}.
\end{flushleft}
\begin{flushleft}
    Der Beschrieb, wie sich der Fehler dann äussert ist hier zu finden:\\
    \hyperref[subsec:appendix_testing_yugabytedb]{Anhang - YugabyteDB Testing}
\end{flushleft}